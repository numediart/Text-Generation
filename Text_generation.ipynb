{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text generation",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSGiT8AZgGMf",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "This notebook analyzes the code given with https://paperswithcode.com/paper/language-models-are-unsupervised-multitask\n",
        "\n",
        "It has been inspired from https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "and https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-pretrained-bert_gpt.ipynb#scrollTo=wcW-1zmUsuff\n",
        "\n",
        "It tries to use the four models as text generation rather than language modeling.\n",
        "\n",
        "The models used are the following ones : \n",
        "\n",
        "1.   Google's BERT model\n",
        "2.   OpenAI's GPT model\n",
        "3.   Google/CMU's Transformer-XL model\n",
        "4.   OpenAI's GPT-2 model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYe6UwmYgGMy",
        "colab_type": "code",
        "outputId": "24333377-f1ec-49a9-d996-0ac2fbcd0967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        }
      },
      "source": [
        "import shutil\n",
        "!pip install regex ftfy pytorch-pretrained-bert\n",
        "!git clone https://github.com/numediart/Text-Generation.git\n",
        "!git clone https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "shutil.move(\"pytorch-pretrained-BERT/examples\", \"examples\")\n",
        "shutil.rmtree(\"pytorch-pretrained-BERT\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.6.8)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (5.5.1)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.175)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.175 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.175)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.6.16)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.175->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.175->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.175->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "fatal: destination path 'Text-Generation' already exists and is not an empty directory.\n",
            "fatal: destination path 'pytorch-pretrained-BERT' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ead5f46eb902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git clone https://github.com/numediart/Text-Generation.git'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git clone https://github.com/huggingface/pytorch-pretrained-BERT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pytorch-pretrained-BERT/examples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"examples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pytorch-pretrained-BERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd Text-Generation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path 'examples/examples' already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwD9Ym2HgGNL",
        "colab_type": "text"
      },
      "source": [
        "### .1 Using Bert Model\n",
        "\n",
        "Basically it can't generate text, it can at most fill in one or a few words in a well-constructed sentence.\n",
        "\n",
        "See http://mayhewsw.github.io/2019/01/16/can-bert-generate-text/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cpfWjiPm9IJ8",
        "outputId": "f4374c94-62cf-44c2-fcb3-9b17efbf1d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "!python Text-Generation/bert.py --text \"All my friends were coming at the party.\" --mask \"friends\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: All my friends were coming at the party.\n",
            "Masked: all my [MASK] were coming at the party .\n",
            "Predicted token: ['parents']\n",
            "Other options:\n",
            "['friends']\n",
            "['kids']\n",
            "['people']\n",
            "['they']\n",
            "['mom']\n",
            "['classes']\n",
            "['girls']\n",
            "['thoughts']\n",
            "['things']\n",
            "['own']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkg_B67n9PmV",
        "colab_type": "text"
      },
      "source": [
        "### .2 Using OpenAI-GPT Model\n",
        "\n",
        "This model can generate text. It's still far from human generation but at least it works.\n",
        "\n",
        "The seed given in \"text\" variable conditionned the generation that will be made. It's a determined generation as it can be reproduced with the same seed. \"tokens_to_generate\" expects an int with number of tokens to output (different from the number of words !)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvnCwCqx9TJ8",
        "colab_type": "code",
        "outputId": "dc9b1d8c-6c0a-4e6d-fb20-2021a183c6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!python Text-Generation/openai.py --text \"Give this a little try.\" --tokens_to_generate 40"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 815973/815973 [00:00<00:00, 859357.79B/s]\n",
            "100% 458495/458495 [00:00<00:00, 612091.36B/s]\n",
            "100% 478750579/478750579 [00:39<00:00, 12062529.25B/s]\n",
            "100% 273/273 [00:00<00:00, 191063.74B/s]\n",
            "give this a little try . \" \n",
            " \" i 'm not sure i can . \" \n",
            " \" you can . \" \n",
            " \" i do n't know . \" \n",
            " \" you can . \" \n",
            " \" i do n't know . \" \n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFo7u2K_AXm",
        "colab_type": "text"
      },
      "source": [
        "### .3.1 Using OpenAI GPT-2 Model\n",
        "\n",
        "This model can as well generate text. But few experimentations showed that outputs are far less interesting that GPT model.\n",
        "The examples regularly come to loop over the same sentences. See below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8aF0Uw1_FvD",
        "colab_type": "code",
        "outputId": "46747419-114a-4586-f1bf-86e468f3ba77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!python Text-Generation/gpt2.py --text \"Maybe this will work\" --tokens_to_generate 120"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maybe this will work for you.\r\n",
            "\r\n",
            "The first thing you need to do is to create a new file called \"config.json\" in your project's root directory.\r\n",
            "\r\n",
            "In this file, you'll need to add the following line to your .bashrc :\r\n",
            "\r\n",
            "{ \"name\": \"config.json\", \"version\": \"1.0\", \"version_id\": \"1\", \"version_name\": \"config.json\", \"version_name_id\": \"1\", \"version_name_name\": \"config.json\", \"version_name_name_id\": \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2tIEbrVDO0u",
        "colab_type": "text"
      },
      "source": [
        "### .3.2 Using provided example code for GPT2\n",
        "\n",
        "\n",
        "Example 'pytorch-pretrained-BERT/examples/run_gpt2.py' provides an interface to enter a seed or add \"--unconditional\" parameter to avoid entering some seed.\n",
        "\n",
        "--nsamples can be used to show more samples with one seed.\n",
        "\n",
        "--length is settable as well.\n",
        "\n",
        "Process is non-determined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmnCzdM65NWH",
        "colab_type": "code",
        "outputId": "c2753ff4-a556-4818-98f2-9e4d61cbe564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python examples/run_gpt2.py --unconditional --nsamples 2 --length 40"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=-1, length=40, model_name_or_path='gpt2', nsamples=2, seed=0, temperature=1.0, top_k=0, unconditional=True)\n",
            "07/08/2019 13:07:20 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.pytorch_pretrained_bert/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "07/08/2019 13:07:20 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.pytorch_pretrained_bert/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "07/08/2019 13:07:21 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "07/08/2019 13:07:21 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.pytorch_pretrained_bert/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
            "07/08/2019 13:07:21 - INFO - pytorch_pretrained_bert.modeling_gpt2 -   Model config {\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "100% 40/40 [00:00<00:00, 42.17it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "3:31 AM - Aug 22, 2015 #1 2015-08-22T12:31\n",
            "\n",
            "> http://pod.netscape.net/jsforge/johnnyglabel\n",
            "100% 40/40 [00:00<00:00, 42.47it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            " how the channel Problems with copies of many different book you're clearly not doing right later access. amazon download http://discord.gg/44AASoYD\n",
            "\n",
            "it is highlighted;\n",
            "================================================================================\n",
            "100% 40/40 [00:00<00:00, 43.13it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "Drop your and accuse me of lying or anything. It would hurt. (kissing pattern) I made you drop your pants and do you like Pierce letting your getting ready in front of us off of\n",
            "100% 40/40 [00:00<00:00, 43.32it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            "NOT AN EXCELLENT DOUBLE AND SURFER README HERE.\n",
            "\n",
            "Updated 8:30 pm EST, 02:12 pm (CST)\n",
            "\n",
            "A Rogue Shadow full RA\n",
            "================================================================================\n",
            "100% 40/40 [00:00<00:00, 43.87it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "Sony, Apain, Lush Blaster, Olympus, Ikopix, Nintendo, Intelsat, ASUS, Lenovo, Sharp, Filistore, Narbel, Blaster JR, Darth Jack,\n",
            "100% 40/40 [00:00<00:00, 44.65it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\"They live in some prisons that don't recognize these problems,\" said Diane, who was in a separate line when Maurita gave her account. \"It's our first time being spoken to. He\n",
            "================================================================================\n",
            "100% 40/40 [00:00<00:00, 43.83it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "in my archives - 1886 Swedish Flavor Special - New York, 2014\n",
            "@_Victor_Schuin |\n",
            "@az115\n",
            "filtered and Bundesstaat der sich geigt\n",
            "100% 40/40 [00:00<00:00, 43.03it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            "It wasn't in her power to approve or deny cleaning up afterwards. That knowledge had a kaleidoscope of charms, and could supplement or destroy her desires. She was suspiciously devoted once the compound\n",
            "================================================================================\n",
            "100% 40/40 [00:00<00:00, 43.62it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\"We can speak normally only because we are members of the Cyprus international perspective and as such have the backpedaling responsibility to ensure the good Standing Committee was unable at its minimum to get a result on\n",
            "100% 40/40 [00:00<00:00, 42.88it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"examples/run_gpt2.py\", line 129, in <module>\n",
            "    run_model()\n",
            "  File \"examples/run_gpt2.py\", line 120, in run_model\n",
            "    out = out[:,1:].tolist()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLKSqpfXW87S",
        "colab_type": "text"
      },
      "source": [
        "### .3.3 Adapting example code for GPT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvr17tpIAPUL",
        "colab_type": "code",
        "outputId": "2af1f6d7-5f9a-47a3-ad4f-a17acb32abac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python Text-Generation/openai_huggingface_example.py --unconditional --length 50"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/08/2019 13:08:40 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /root/.pytorch_pretrained_bert/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
            "07/08/2019 13:08:40 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /root/.pytorch_pretrained_bert/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
            "07/08/2019 13:08:43 - INFO - pytorch_pretrained_bert.modeling_openai -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
            "07/08/2019 13:08:43 - INFO - pytorch_pretrained_bert.modeling_openai -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /root/.pytorch_pretrained_bert/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.f59b19eb0e361a0230a1106b66b8c6e7a994cb200cd63d9190cda8d56d75ff85\n",
            "07/08/2019 13:08:43 - INFO - pytorch_pretrained_bert.modeling_openai -   Model config {\n",
            "  \"afn\": \"gelu\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 512,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 512,\n",
            "  \"n_special\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"vocab_size\": 40478\n",
            "}\n",
            "\n",
            "Using start word :unner\n",
            "100% 50/50 [00:01<00:00, 43.74it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "r he became so angry, so the really controlled the raging incubus, and in wr he lay whining and wifery,'i gimme more of t'ain't t'caash next long...'\" \n",
            " kiersten smiled at me. \" i\n",
            "================================================================================\n",
            "Using start word :stephens</w>\n",
            "100% 50/50 [00:01<00:00, 43.86it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", but when i look back up at the desk, nothing but his violet eyes visible. he guards a beast on account of cash. i swing the journal into a butcher's block and slide it into my jacket pocket. another uniformed soldier watches me with\n",
            "================================================================================\n",
            "Using start word :alligator</w>\n",
            "100% 50/50 [00:01<00:00, 43.90it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "and this guy had shot my friend. the premier fellow took a drink, wiped his mouth on his sleeve, and then proceeded to make a scene, noisily retching into his handkerchief. at that point, we were elated with the dejected approach to telling the\n",
            "================================================================================\n",
            "Using start word :lingui\n",
            "100% 50/50 [00:01<00:00, 44.16it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "tiferous ( if i may say so myself ). many in the known world emerge from myriad shops for reasons only i can possibly understand, and even beyond that there is little reason to keep them here. yet, women tend to dwarf them,\n",
            "================================================================================\n",
            "Using start word :uncover</w>\n",
            "100% 50/50 [00:01<00:00, 44.27it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". if there was no guarantee that their family needs something to feed in return. but the only thing that is particularly disconcerting is that they are at some point in their life when they keep the place they love their oldest child unattended any longer. they are\n",
            "================================================================================\n",
            "Using start word :leggings</w>\n",
            "100% 50/50 [00:01<00:00, 44.10it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", the house's one thing but the neck in the back, a true uniform dress. sazed stepped back into view, keeping a watch as the men pressed forward, their weapons in hand, heads down and never clearing the walls. \n",
            " \" hamish!\n",
            "================================================================================\n",
            "Using start word :picnic\n",
            "100% 50/50 [00:01<00:00, 43.48it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "vedly exhibited. the agents directed me to the surgery, where a wailing nora storm resided. \n",
            " \" have you lost some clothing!? \" she asked, snatching it right out of my hands. \" oh my goodness, look! \" \n",
            " she\n",
            "================================================================================\n",
            "Using start word :yas\n",
            "100% 50/50 [00:01<00:00, 45.42it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "sitzen prisons are certainly nothing i have not seen before on the surface ( and, incidentally, thanks to television, old housewives never try to describe a dungeon ). there's no glass inside this room, no glass in the glass toe of that\n",
            "================================================================================\n",
            "Using start word :bombed</w>\n",
            "100% 50/50 [00:01<00:00, 45.25it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "with thousands of dead soldiers and had to add another must have gotten in somehow. \n",
            " dax wasn't exactly quiet as he dropped through the misshapen ceiling, twenty feet below. the dust got under his shoes and glued his rags to his skin. he quickly\n",
            "================================================================================\n",
            "Using start word :rapped</w>\n",
            "100% 50/50 [00:01<00:00, 44.35it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "in the double doors. \n",
            " too deaf to hear it, he kicked away the plywood and peeked out the door. \n",
            " an enormous line of people stood outside the suits, among them tweety bird, the rundown hairdressers, and the salvation army\n",
            "================================================================================\n",
            "Using start word :pinkie</w>\n",
            "100% 50/50 [00:01<00:00, 44.48it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". \n",
            " \" okay, okay. \" i bit my tongue, sweat beading on my brow from the effort. \" here goes. \" i took my phone out, scrolling to google 158. there it was. i clicked on the search box.\n",
            "================================================================================\n",
            "Using start word :embroi\n",
            "100% 50/50 [00:01<00:00, 44.82it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "sh her thin lips belt. \" i don't know. perhaps westfield thought you would prefer someone this... flashier. ah... like... \" \n",
            " \" meetcha, luv. \" he chuckled as he stood, bumping his shoulder against her. he\n",
            "================================================================================\n",
            "Using start word :ggily</w>\n",
            "100% 50/50 [00:01<00:00, 44.54it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "dent with the spade and hauled it onto the board. \n",
            " billy dropped the spade in the sand and bent down to pick up a small, old see - through, wooden block. he picked it up, smiled at the archeologist, and turned away\n",
            "================================================================================\n",
            "Using start word :wagged</w>\n",
            "100% 50/50 [00:01<00:00, 43.86it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "the flag around on his breast. \" justice alliance. \" \n",
            " the hollows around her eyes hardened. she hated that word, hated the use of it, hated that it had been used against her again. no, she never loved justice alliance, but\n",
            "================================================================================\n",
            "Using start word :essay</w>\n",
            "100% 50/50 [00:01<00:00, 44.34it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "more. \" \n",
            " he blinked at me in question. \n",
            " it was another thing i loved about will. he didn't bullshit. \n",
            " i picked up two graham crackers from the platter. \" want some? \" \n",
            " \" that would be perfect. \"\n",
            "================================================================================\n",
            "Using start word :sutherland</w>\n",
            "100% 50/50 [00:01<00:00, 43.22it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "made a point of seeing to its needs. but how long could he do this for? ashlar would have less than a month, at the most, before he needed to take up a new position. sebastian would be banished to some vast old estate\n",
            "================================================================================\n",
            "Using start word :gaily</w>\n",
            "100% 50/50 [00:01<00:00, 43.02it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "breath of cold air on the bed. \n",
            " \" he doesn't so much as twitch ; sit up! \" \n",
            " roger's eyes flew open, startled. \n",
            " \" thank who? \" \n",
            " \" who what? \" \n",
            " \" who is the'he\n",
            "================================================================================\n",
            "Using start word :picks</w>\n",
            "100% 50/50 [00:01<00:00, 44.76it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "through the curtains, settling on the faint image of her, curled up in her corner of the closet. there is no sound coming from her. she is asleep. \n",
            " two hours. she has gone to sleep. \n",
            " my fears trickle out of me\n",
            "================================================================================\n",
            "Using start word :repet\n",
            "100% 50/50 [00:01<00:00, 44.21it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "io i have gleaned from my observations that this is a good place to hide. we are in some swamps and i can not avoid stopping this here song because of the moisture. \" \n",
            " all listeners were silent. \n",
            " \" so where you from? \"\n",
            "================================================================================\n",
            "Using start word :surf\n",
            "100% 50/50 [00:01<00:00, 44.69it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "devil, doing whatever i asked, you illiterate brat, you. \" spear slammed the door shut, and returned grandiose to his wardrobe, returning to his tall, sinewy body. he laughed a little, and then slew a sober looking man who had\n",
            "================================================================================\n",
            "Using start word :beijing</w>\n",
            "100% 50/50 [00:01<00:00, 43.03it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". \" \n",
            " \" no ethan. kyle was at the game. kyle noticed that we were there, and he showed up. \" \n",
            " \" kyle was there before? \" \n",
            " \" he was at the game, but not the next time. that's\n",
            "================================================================================\n",
            "Using start word :anter\n",
            "100% 50/50 [00:01<00:00, 45.59it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "lings of the help to his mind that he should remain here and wait until the angaraks and i at least had our armor on. odiana would have advised me to remain out of sight, but when i spoke with him this morning he said\n",
            "================================================================================\n",
            "Using start word :brutally</w>\n",
            "100% 50/50 [00:01<00:00, 43.42it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "warred between its brothers'paranoia and the desire to throw a budding army of zombies over its own head - the drones of the apocalypse had no reason to run from zombies. they didn't think of themselves as gods, to fall like every other species of\n",
            "================================================================================\n",
            "Using start word :marwan</w>\n",
            " 40% 20/50 [00:00<00:00, 45.00it/s]Exception ignored in: <bound method tqdm.__del__ of  40% 20/50 [00:00<00:00, 45.00it/s]>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 931, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 1133, in close\n",
            "    self._decr_instances(self)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\", line 488, in _decr_instances\n",
            "    for inst in cls._instances:\n",
            "  File \"/usr/lib/python3.6/_weakrefset.py\", line 59, in __iter__\n",
            "    with _IterationGuard(self):\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"Text-Generation/openai_huggingface_example.py\", line 140, in <module>\n",
            "    run_model()\n",
            "  File \"Text-Generation/openai_huggingface_example.py\", line 128, in run_model\n",
            "    temperature=args.temperature, top_k=args.top_k, device=device\n",
            "  File \"Text-Generation/openai_huggingface_example.py\", line 47, in sample_sequence\n",
            "    logits = model(prev)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling_openai.py\", line 718, in forward\n",
            "    hidden_states = self.transformer(input_ids, position_ids, token_type_ids)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling_openai.py\", line 643, in forward\n",
            "    hidden_states = block(hidden_states)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling_openai.py\", line 334, in forward\n",
            "    a = self.attn(x)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 493, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling_openai.py\", line 302, in forward\n",
            "    a = self._attn(query, key, value)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling_openai.py\", line 277, in _attn\n",
            "    w = w * b + -1e9 * (1 - b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 371, in __rsub__\n",
            "    return _C._VariableFunctions.rsub(self, other)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IqrDZzlD0Fd",
        "colab_type": "text"
      },
      "source": [
        "### .4 Using Transformer-XL Model\n",
        "\n",
        "Model has been trained on wiki103 which is based on wikipedia pages so it will surely output text in this format.\n",
        "\n",
        "Code has been copied and modified from https://github.com/kimiyoung/transformer-xl/issues/49#issuecomment-472212730.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc3nkJiXIEn8",
        "colab_type": "code",
        "outputId": "c1f40945-3713-490b-d789-7f2d810cffd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "source": [
        "!python Text-Generation/transformer_xl.py --text \"First world war\" --tokens_to_generate 200"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization_transfo_xl:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin from cache at /root/.pytorch_pretrained_bert/b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
            "INFO:pytorch_pretrained_bert.modeling_transfo_xl:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/12642ff7d0279757d8356bfd86a729d9697018a0c93ad042de1d0d2cc17fd57b.e9704971f27275ec067a00a67e6a5f0b05b4306b3f714a96e9f763d8fb612671\n",
            "INFO:pytorch_pretrained_bert.modeling_transfo_xl:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json from cache at /root/.pytorch_pretrained_bert/a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.aef76fb1064c932cd6a2a2be3f23ebbfa5f9b6e29e8e87b571c45b4a5d5d1b90\n",
            "INFO:pytorch_pretrained_bert.modeling_transfo_xl:Model config {\n",
            "  \"adaptive\": true,\n",
            "  \"attn_type\": 0,\n",
            "  \"clamp_len\": 1000,\n",
            "  \"cutoffs\": [\n",
            "    20000,\n",
            "    40000,\n",
            "    200000\n",
            "  ],\n",
            "  \"d_embed\": 1024,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"div_val\": 4,\n",
            "  \"dropatt\": 0.0,\n",
            "  \"dropout\": 0.1,\n",
            "  \"ext_len\": 0,\n",
            "  \"init\": \"normal\",\n",
            "  \"init_range\": 0.01,\n",
            "  \"init_std\": 0.02,\n",
            "  \"mem_len\": 1600,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 18,\n",
            "  \"n_token\": 267735,\n",
            "  \"pre_lnorm\": false,\n",
            "  \"proj_init_std\": 0.01,\n",
            "  \"same_length\": true,\n",
            "  \"sample_softmax\": -1,\n",
            "  \"tgt_len\": 128,\n",
            "  \"tie_projs\": [\n",
            "    false,\n",
            "    true,\n",
            "    true,\n",
            "    true\n",
            "  ],\n",
            "  \"tie_weight\": true,\n",
            "  \"untie_r\": true\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "= I Just Can 't Stop Believing You 're Getting to Stop = an The Star-Ledger-Enquirer article about him titled \" The Sun Rises About The Stars \". \n",
            "\n",
            "= = Early career and life = = \n",
            "\n",
            "Alan Moore was born in Manchester, England, to parents George Arthur Moore and Edith Moore. He studied English at St John's College, Cambridge, and served first in the Royal Marines during World War I, and then in the Royal Air Force for four years. While stationed at Yeovilton, he met Edith Moore, who later recounted their initial meeting: \" I went and sat down to read novels, and said to her, ' I would like you to stay behind for two days, take my exams, and do anything else. ' But the last time I returned the next day, Edith wrote down a second thing she said, ' I 'd like you to leave me alone. ' \" She eventually made it to England, where she married Moore and they had two children \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyMG76NUkmgs",
        "colab_type": "text"
      },
      "source": [
        "### .5 Fine-tuning Data\n",
        "\n",
        "If you want to customize your text generation, you can fine-tune a GPT model.\n",
        "\n",
        "Code has been taken from 'pytorch-pretrained-BERT/examples/run-openai-gpt.py' and customized to fit one-class data in utf8 format and txt file.\n",
        "\n",
        "Upload a txt file with each sample on one line (for example, one line = one paragraph) then modify command line to adapt your file name and save directory.\n",
        "\n",
        "When trained, go back to .3.3 and use your save directory name as model path to start generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1zFT_bt93xI",
        "colab_type": "code",
        "outputId": "25a93349-6635-40ef-b463-2d4fa9c3846e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python Text-Generation/fine_tuning_openai.py --do_train --output_dir train_clarke --train_dataset Text-Generation/FineTuning-example.txt"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(do_eval=False, do_train=True, eval_batch_size=16, eval_dataset='', learning_rate=6.25e-05, lm_coef=0.9, lr_schedule='warmup_linear', max_grad_norm=1, model_name='openai-gpt', n_valid=374, num_train_epochs=3, output_dir='train_clarke', seed=42, server_ip='', server_port='', train_batch_size=8, train_dataset='Text-Generation/FineTuning-example.txt', warmup_proportion=0.002, weight_decay=0.01)\n",
            "07/08/2019 13:22:21 - INFO - __main__ -   device: cuda, n_gpu 1\n",
            "07/08/2019 13:22:23 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /root/.pytorch_pretrained_bert/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
            "07/08/2019 13:22:23 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /root/.pytorch_pretrained_bert/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
            "07/08/2019 13:22:24 - INFO - pytorch_pretrained_bert.tokenization_openai -   Special tokens {'_start_': 40478, '_end_': 40479}\n",
            "07/08/2019 13:22:26 - INFO - pytorch_pretrained_bert.modeling_openai -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /root/.pytorch_pretrained_bert/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\n",
            "07/08/2019 13:22:26 - INFO - pytorch_pretrained_bert.modeling_openai -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /root/.pytorch_pretrained_bert/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.f59b19eb0e361a0230a1106b66b8c6e7a994cb200cd63d9190cda8d56d75ff85\n",
            "07/08/2019 13:22:26 - INFO - pytorch_pretrained_bert.modeling_openai -   Model config {\n",
            "  \"afn\": \"gelu\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 512,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 512,\n",
            "  \"n_special\": 0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"vocab_size\": 40478\n",
            "}\n",
            "\n",
            "07/08/2019 13:22:32 - INFO - __main__ -   Encoding dataset...\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Training:   0% 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Training loss: 3.62e+00 lr: 6.25e-05:   1% 1/157 [00:01<03:02,  1.17s/it]\u001b[A\n",
            "Training loss: 3.55e+00 lr: 6.24e-05:   1% 2/157 [00:02<02:56,  1.14s/it]\u001b[A\n",
            "Training loss: 3.54e+00 lr: 6.22e-05:   2% 3/157 [00:03<02:53,  1.13s/it]\u001b[A\n",
            "Training loss: 3.50e+00 lr: 6.21e-05:   3% 4/157 [00:04<02:50,  1.11s/it]\u001b[A\n",
            "Training loss: 3.47e+00 lr: 6.20e-05:   3% 5/157 [00:05<02:48,  1.11s/it]\u001b[A\n",
            "Training loss: 3.41e+00 lr: 6.18e-05:   4% 6/157 [00:06<02:45,  1.10s/it]\u001b[A\n",
            "Training loss: 3.31e+00 lr: 6.17e-05:   4% 7/157 [00:07<02:43,  1.09s/it]\u001b[A\n",
            "Training loss: 3.33e+00 lr: 6.16e-05:   5% 8/157 [00:08<02:42,  1.09s/it]\u001b[A\n",
            "Training loss: 3.30e+00 lr: 6.14e-05:   6% 9/157 [00:09<02:41,  1.09s/it]\u001b[A\n",
            "Training loss: 3.34e+00 lr: 6.13e-05:   6% 10/157 [00:10<02:39,  1.09s/it]\u001b[A\n",
            "Training loss: 3.29e+00 lr: 6.12e-05:   7% 11/157 [00:11<02:38,  1.08s/it]\u001b[A\n",
            "Training loss: 3.33e+00 lr: 6.10e-05:   8% 12/157 [00:13<02:36,  1.08s/it]\u001b[A\n",
            "Training loss: 3.31e+00 lr: 6.09e-05:   8% 13/157 [00:14<02:36,  1.09s/it]\u001b[A\n",
            "Training loss: 3.33e+00 lr: 6.08e-05:   9% 14/157 [00:15<02:36,  1.09s/it]\u001b[A\n",
            "Training loss: 3.33e+00 lr: 6.06e-05:  10% 15/157 [00:16<02:34,  1.09s/it]\u001b[A\n",
            "Training loss: 3.41e+00 lr: 6.05e-05:  10% 16/157 [00:17<02:33,  1.09s/it]\u001b[A\n",
            "Training loss: 3.38e+00 lr: 6.04e-05:  11% 17/157 [00:18<02:32,  1.09s/it]\u001b[A\n",
            "Training loss: 3.33e+00 lr: 6.02e-05:  11% 18/157 [00:19<02:30,  1.08s/it]\u001b[A\n",
            "Training loss: 3.30e+00 lr: 6.01e-05:  12% 19/157 [00:20<02:29,  1.08s/it]\u001b[A\n",
            "Training loss: 3.30e+00 lr: 6.00e-05:  13% 20/157 [00:21<02:28,  1.08s/it]\u001b[A\n",
            "Training loss: 3.21e+00 lr: 5.98e-05:  13% 21/157 [00:22<02:27,  1.09s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 5.97e-05:  14% 22/157 [00:23<02:26,  1.08s/it]\u001b[A\n",
            "Training loss: 3.22e+00 lr: 5.96e-05:  15% 23/157 [00:25<02:24,  1.08s/it]\u001b[A\n",
            "Training loss: 3.26e+00 lr: 5.94e-05:  15% 24/157 [00:26<02:24,  1.09s/it]\u001b[A\n",
            "Training loss: 3.22e+00 lr: 5.93e-05:  16% 25/157 [00:27<02:23,  1.08s/it]\u001b[A\n",
            "Training loss: 3.24e+00 lr: 5.92e-05:  17% 26/157 [00:28<02:22,  1.08s/it]\u001b[A\n",
            "Training loss: 3.31e+00 lr: 5.90e-05:  17% 27/157 [00:29<02:21,  1.09s/it]\u001b[A\n",
            "Training loss: 3.30e+00 lr: 5.89e-05:  18% 28/157 [00:30<02:19,  1.08s/it]\u001b[A\n",
            "Training loss: 3.27e+00 lr: 5.88e-05:  18% 29/157 [00:31<02:18,  1.08s/it]\u001b[A\n",
            "Training loss: 3.22e+00 lr: 5.86e-05:  19% 30/157 [00:32<02:18,  1.09s/it]\u001b[A\n",
            "Training loss: 3.20e+00 lr: 5.85e-05:  20% 31/157 [00:33<02:17,  1.09s/it]\u001b[A\n",
            "Training loss: 3.25e+00 lr: 5.84e-05:  20% 32/157 [00:34<02:16,  1.09s/it]\u001b[A\n",
            "Training loss: 3.21e+00 lr: 5.82e-05:  21% 33/157 [00:35<02:15,  1.09s/it]\u001b[A\n",
            "Training loss: 3.24e+00 lr: 5.81e-05:  22% 34/157 [00:36<02:13,  1.09s/it]\u001b[A\n",
            "Training loss: 3.20e+00 lr: 5.80e-05:  22% 35/157 [00:38<02:12,  1.09s/it]\u001b[A\n",
            "Training loss: 3.24e+00 lr: 5.78e-05:  23% 36/157 [00:39<02:12,  1.09s/it]\u001b[A\n",
            "Training loss: 3.24e+00 lr: 5.77e-05:  24% 37/157 [00:40<02:10,  1.09s/it]\u001b[A\n",
            "Training loss: 3.25e+00 lr: 5.76e-05:  24% 38/157 [00:41<02:09,  1.09s/it]\u001b[A\n",
            "Training loss: 3.31e+00 lr: 5.74e-05:  25% 39/157 [00:42<02:08,  1.09s/it]\u001b[A\n",
            "Training loss: 3.29e+00 lr: 5.73e-05:  25% 40/157 [00:43<02:07,  1.09s/it]\u001b[A\n",
            "Training loss: 3.22e+00 lr: 5.72e-05:  26% 41/157 [00:44<02:07,  1.10s/it]\u001b[A\n",
            "Training loss: 3.19e+00 lr: 5.70e-05:  27% 42/157 [00:45<02:05,  1.09s/it]\u001b[A\n",
            "Training loss: 3.21e+00 lr: 5.69e-05:  27% 43/157 [00:46<02:03,  1.09s/it]\u001b[A\n",
            "Training loss: 3.22e+00 lr: 5.68e-05:  28% 44/157 [00:47<02:02,  1.09s/it]\u001b[A\n",
            "Training loss: 3.19e+00 lr: 5.66e-05:  29% 45/157 [00:48<02:01,  1.09s/it]\u001b[A\n",
            "Training loss: 3.15e+00 lr: 5.65e-05:  29% 46/157 [00:50<02:00,  1.09s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 5.64e-05:  30% 47/157 [00:51<02:00,  1.09s/it]\u001b[A\n",
            "Training loss: 3.05e+00 lr: 5.62e-05:  31% 48/157 [00:52<01:59,  1.09s/it]\u001b[A\n",
            "Training loss: 3.04e+00 lr: 5.61e-05:  31% 49/157 [00:53<01:58,  1.10s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 5.60e-05:  32% 50/157 [00:54<01:57,  1.09s/it]\u001b[A\n",
            "Training loss: 3.14e+00 lr: 5.58e-05:  32% 51/157 [00:55<01:55,  1.09s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 5.57e-05:  33% 52/157 [00:56<01:54,  1.09s/it]\u001b[A\n",
            "Training loss: 3.09e+00 lr: 5.56e-05:  34% 53/157 [00:57<01:53,  1.09s/it]\u001b[A\n",
            "Training loss: 3.14e+00 lr: 5.54e-05:  34% 54/157 [00:58<01:52,  1.09s/it]\u001b[A\n",
            "Training loss: 3.14e+00 lr: 5.53e-05:  35% 55/157 [00:59<01:51,  1.10s/it]\u001b[A\n",
            "Training loss: 3.16e+00 lr: 5.52e-05:  36% 56/157 [01:01<01:50,  1.09s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 5.50e-05:  36% 57/157 [01:02<01:49,  1.09s/it]\u001b[A\n",
            "Training loss: 3.17e+00 lr: 5.49e-05:  37% 58/157 [01:03<01:48,  1.09s/it]\u001b[A\n",
            "Training loss: 3.15e+00 lr: 5.48e-05:  38% 59/157 [01:04<01:46,  1.09s/it]\u001b[A\n",
            "Training loss: 3.14e+00 lr: 5.46e-05:  38% 60/157 [01:05<01:45,  1.09s/it]\u001b[A\n",
            "Training loss: 3.18e+00 lr: 5.45e-05:  39% 61/157 [01:06<01:44,  1.09s/it]\u001b[A\n",
            "Training loss: 3.21e+00 lr: 5.44e-05:  39% 62/157 [01:07<01:43,  1.09s/it]\u001b[A\n",
            "Training loss: 3.28e+00 lr: 5.42e-05:  40% 63/157 [01:08<01:42,  1.09s/it]\u001b[A\n",
            "Training loss: 3.22e+00 lr: 5.41e-05:  41% 64/157 [01:09<01:41,  1.09s/it]\u001b[A\n",
            "Training loss: 3.21e+00 lr: 5.40e-05:  41% 65/157 [01:10<01:40,  1.10s/it]\u001b[A\n",
            "Training loss: 3.17e+00 lr: 5.38e-05:  42% 66/157 [01:11<01:39,  1.09s/it]\u001b[A\n",
            "Training loss: 3.19e+00 lr: 5.37e-05:  43% 67/157 [01:13<01:38,  1.09s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 5.36e-05:  43% 68/157 [01:14<01:37,  1.10s/it]\u001b[A\n",
            "Training loss: 3.10e+00 lr: 5.35e-05:  44% 69/157 [01:15<01:36,  1.10s/it]\u001b[A\n",
            "Training loss: 3.06e+00 lr: 5.33e-05:  45% 70/157 [01:16<01:34,  1.09s/it]\u001b[A\n",
            "Training loss: 3.18e+00 lr: 5.32e-05:  45% 71/157 [01:17<01:33,  1.09s/it]\u001b[A\n",
            "Training loss: 3.20e+00 lr: 5.31e-05:  46% 72/157 [01:18<01:32,  1.09s/it]\u001b[A\n",
            "Training loss: 3.15e+00 lr: 5.29e-05:  46% 73/157 [01:19<01:31,  1.09s/it]\u001b[A\n",
            "Training loss: 3.18e+00 lr: 5.28e-05:  47% 74/157 [01:20<01:30,  1.10s/it]\u001b[A\n",
            "Training loss: 3.23e+00 lr: 5.27e-05:  48% 75/157 [01:21<01:29,  1.09s/it]\u001b[A\n",
            "Training loss: 3.16e+00 lr: 5.25e-05:  48% 76/157 [01:22<01:28,  1.09s/it]\u001b[A\n",
            "Training loss: 3.16e+00 lr: 5.24e-05:  49% 77/157 [01:23<01:27,  1.09s/it]\u001b[A\n",
            "Training loss: 3.18e+00 lr: 5.23e-05:  50% 78/157 [01:25<01:25,  1.09s/it]\u001b[A\n",
            "Training loss: 3.22e+00 lr: 5.21e-05:  50% 79/157 [01:26<01:24,  1.09s/it]\u001b[A\n",
            "Training loss: 3.16e+00 lr: 5.20e-05:  51% 80/157 [01:27<01:23,  1.09s/it]\u001b[A\n",
            "Training loss: 3.19e+00 lr: 5.19e-05:  52% 81/157 [01:28<01:23,  1.10s/it]\u001b[A\n",
            "Training loss: 3.19e+00 lr: 5.17e-05:  52% 82/157 [01:29<01:22,  1.10s/it]\u001b[A\n",
            "Training loss: 3.29e+00 lr: 5.16e-05:  53% 83/157 [01:30<01:21,  1.10s/it]\u001b[A\n",
            "Training loss: 3.43e+00 lr: 5.15e-05:  54% 84/157 [01:31<01:20,  1.10s/it]\u001b[A\n",
            "Training loss: 3.34e+00 lr: 5.13e-05:  54% 85/157 [01:32<01:18,  1.10s/it]\u001b[A\n",
            "Training loss: 3.31e+00 lr: 5.12e-05:  55% 86/157 [01:33<01:17,  1.09s/it]\u001b[A\n",
            "Training loss: 3.24e+00 lr: 5.11e-05:  55% 87/157 [01:34<01:16,  1.09s/it]\u001b[A\n",
            "Training loss: 3.21e+00 lr: 5.09e-05:  56% 88/157 [01:35<01:15,  1.09s/it]\u001b[A\n",
            "Training loss: 3.20e+00 lr: 5.08e-05:  57% 89/157 [01:37<01:14,  1.09s/it]\u001b[A\n",
            "Training loss: 3.15e+00 lr: 5.07e-05:  57% 90/157 [01:38<01:13,  1.09s/it]\u001b[A\n",
            "Training loss: 3.18e+00 lr: 5.05e-05:  58% 91/157 [01:39<01:11,  1.09s/it]\u001b[A\n",
            "Training loss: 3.23e+00 lr: 5.04e-05:  59% 92/157 [01:40<01:10,  1.09s/it]\u001b[A\n",
            "Training loss: 3.20e+00 lr: 5.03e-05:  59% 93/157 [01:41<01:09,  1.09s/it]\u001b[A\n",
            "Training loss: 3.26e+00 lr: 5.01e-05:  60% 94/157 [01:42<01:08,  1.09s/it]\u001b[A\n",
            "Training loss: 3.23e+00 lr: 5.00e-05:  61% 95/157 [01:43<01:07,  1.09s/it]\u001b[A\n",
            "Training loss: 3.17e+00 lr: 4.99e-05:  61% 96/157 [01:44<01:06,  1.09s/it]\u001b[A\n",
            "Training loss: 3.09e+00 lr: 4.97e-05:  62% 97/157 [01:45<01:05,  1.09s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 4.96e-05:  62% 98/157 [01:46<01:04,  1.09s/it]\u001b[A\n",
            "Training loss: 3.12e+00 lr: 4.95e-05:  63% 99/157 [01:47<01:03,  1.09s/it]\u001b[A\n",
            "Training loss: 3.08e+00 lr: 4.93e-05:  64% 100/157 [01:49<01:02,  1.09s/it]\u001b[A\n",
            "Training loss: 3.08e+00 lr: 4.92e-05:  64% 101/157 [01:50<01:01,  1.09s/it]\u001b[A\n",
            "Training loss: 3.07e+00 lr: 4.91e-05:  65% 102/157 [01:51<00:59,  1.09s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 4.89e-05:  66% 103/157 [01:52<00:58,  1.09s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 4.88e-05:  66% 104/157 [01:53<00:57,  1.09s/it]\u001b[A\n",
            "Training loss: 3.09e+00 lr: 4.87e-05:  67% 105/157 [01:54<00:56,  1.09s/it]\u001b[A\n",
            "Training loss: 3.06e+00 lr: 4.85e-05:  68% 106/157 [01:55<00:55,  1.10s/it]\u001b[A\n",
            "Training loss: 3.05e+00 lr: 4.84e-05:  68% 107/157 [01:56<00:55,  1.10s/it]\u001b[A\n",
            "Training loss: 3.05e+00 lr: 4.83e-05:  69% 108/157 [01:57<00:53,  1.10s/it]\u001b[A\n",
            "Training loss: 3.05e+00 lr: 4.81e-05:  69% 109/157 [01:58<00:52,  1.10s/it]\u001b[A\n",
            "Training loss: 3.01e+00 lr: 4.80e-05:  70% 110/157 [01:59<00:51,  1.10s/it]\u001b[A\n",
            "Training loss: 2.96e+00 lr: 4.79e-05:  71% 111/157 [02:01<00:50,  1.09s/it]\u001b[A\n",
            "Training loss: 3.00e+00 lr: 4.77e-05:  71% 112/157 [02:02<00:49,  1.09s/it]\u001b[A\n",
            "Training loss: 2.95e+00 lr: 4.76e-05:  72% 113/157 [02:03<00:47,  1.09s/it]\u001b[A\n",
            "Training loss: 3.01e+00 lr: 4.75e-05:  73% 114/157 [02:04<00:46,  1.09s/it]\u001b[A\n",
            "Training loss: 3.03e+00 lr: 4.73e-05:  73% 115/157 [02:05<00:45,  1.09s/it]\u001b[A\n",
            "Training loss: 3.06e+00 lr: 4.72e-05:  74% 116/157 [02:06<00:44,  1.09s/it]\u001b[A\n",
            "Training loss: 3.07e+00 lr: 4.71e-05:  75% 117/157 [02:07<00:43,  1.09s/it]\u001b[A\n",
            "Training loss: 3.09e+00 lr: 4.69e-05:  75% 118/157 [02:08<00:42,  1.09s/it]\u001b[A\n",
            "Training loss: 3.08e+00 lr: 4.68e-05:  76% 119/157 [02:09<00:41,  1.09s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 4.67e-05:  76% 120/157 [02:10<00:40,  1.09s/it]\u001b[A\n",
            "Training loss: 3.16e+00 lr: 4.65e-05:  77% 121/157 [02:11<00:39,  1.09s/it]\u001b[A\n",
            "Training loss: 3.15e+00 lr: 4.64e-05:  78% 122/157 [02:13<00:38,  1.09s/it]\u001b[A\n",
            "Training loss: 3.16e+00 lr: 4.63e-05:  78% 123/157 [02:14<00:37,  1.09s/it]\u001b[A\n",
            "Training loss: 3.19e+00 lr: 4.61e-05:  79% 124/157 [02:15<00:35,  1.09s/it]\u001b[A\n",
            "Training loss: 3.19e+00 lr: 4.60e-05:  80% 125/157 [02:16<00:35,  1.09s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 4.59e-05:  80% 126/157 [02:17<00:33,  1.09s/it]\u001b[A\n",
            "Training loss: 3.18e+00 lr: 4.57e-05:  81% 127/157 [02:18<00:32,  1.09s/it]\u001b[A\n",
            "Training loss: 3.08e+00 lr: 4.56e-05:  82% 128/157 [02:19<00:31,  1.09s/it]\u001b[A\n",
            "Training loss: 3.07e+00 lr: 4.55e-05:  82% 129/157 [02:20<00:30,  1.09s/it]\u001b[A\n",
            "Training loss: 3.05e+00 lr: 4.53e-05:  83% 130/157 [02:21<00:29,  1.09s/it]\u001b[A\n",
            "Training loss: 3.15e+00 lr: 4.52e-05:  83% 131/157 [02:22<00:28,  1.09s/it]\u001b[A\n",
            "Training loss: 3.12e+00 lr: 4.51e-05:  84% 132/157 [02:23<00:27,  1.09s/it]\u001b[A\n",
            "Training loss: 3.14e+00 lr: 4.49e-05:  85% 133/157 [02:25<00:26,  1.09s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 4.48e-05:  85% 134/157 [02:26<00:25,  1.09s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 4.47e-05:  86% 135/157 [02:27<00:24,  1.09s/it]\u001b[A\n",
            "Training loss: 3.09e+00 lr: 4.45e-05:  87% 136/157 [02:28<00:22,  1.09s/it]\u001b[A\n",
            "Training loss: 3.09e+00 lr: 4.44e-05:  87% 137/157 [02:29<00:21,  1.09s/it]\u001b[A\n",
            "Training loss: 2.99e+00 lr: 4.43e-05:  88% 138/157 [02:30<00:20,  1.09s/it]\u001b[A\n",
            "Training loss: 3.04e+00 lr: 4.41e-05:  89% 139/157 [02:31<00:19,  1.09s/it]\u001b[A\n",
            "Training loss: 3.08e+00 lr: 4.40e-05:  89% 140/157 [02:32<00:18,  1.09s/it]\u001b[A\n",
            "Training loss: 3.08e+00 lr: 4.39e-05:  90% 141/157 [02:33<00:17,  1.10s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 4.37e-05:  90% 142/157 [02:34<00:16,  1.10s/it]\u001b[A\n",
            "Training loss: 3.17e+00 lr: 4.36e-05:  91% 143/157 [02:36<00:15,  1.10s/it]\u001b[A\n",
            "Training loss: 3.15e+00 lr: 4.35e-05:  92% 144/157 [02:37<00:14,  1.10s/it]\u001b[A\n",
            "Training loss: 3.25e+00 lr: 4.33e-05:  92% 145/157 [02:38<00:13,  1.10s/it]\u001b[A\n",
            "Training loss: 3.24e+00 lr: 4.32e-05:  93% 146/157 [02:39<00:12,  1.10s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 4.31e-05:  94% 147/157 [02:40<00:11,  1.10s/it]\u001b[A\n",
            "Training loss: 3.11e+00 lr: 4.29e-05:  94% 148/157 [02:41<00:09,  1.10s/it]\u001b[A\n",
            "Training loss: 3.06e+00 lr: 4.28e-05:  95% 149/157 [02:42<00:08,  1.09s/it]\u001b[A\n",
            "Training loss: 3.10e+00 lr: 4.27e-05:  96% 150/157 [02:43<00:07,  1.09s/it]\u001b[A\n",
            "Training loss: 3.07e+00 lr: 4.25e-05:  96% 151/157 [02:44<00:06,  1.09s/it]\u001b[A\n",
            "Training loss: 3.12e+00 lr: 4.24e-05:  97% 152/157 [02:45<00:05,  1.09s/it]\u001b[A\n",
            "Training loss: 3.10e+00 lr: 4.23e-05:  97% 153/157 [02:46<00:04,  1.10s/it]\u001b[A\n",
            "Training loss: 3.08e+00 lr: 4.21e-05:  98% 154/157 [02:48<00:03,  1.10s/it]\u001b[A\n",
            "Training loss: 3.20e+00 lr: 4.20e-05:  99% 155/157 [02:49<00:02,  1.10s/it]\u001b[A\n",
            "Training loss: 3.16e+00 lr: 4.19e-05:  99% 156/157 [02:50<00:01,  1.10s/it]\u001b[A\n",
            "Training loss: 3.13e+00 lr: 4.18e-05: 100% 157/157 [02:51<00:00,  1.09s/it]\u001b[A\n",
            "Epoch:  33% 1/3 [02:51<05:42, 171.36s/it]\n",
            "Training:   0% 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Training loss: 3.00e+00 lr: 4.16e-05:   1% 1/157 [00:01<02:50,  1.09s/it]\u001b[A\n",
            "Training loss: 2.79e+00 lr: 4.15e-05:   1% 2/157 [00:02<02:49,  1.09s/it]\u001b[A\n",
            "Training loss: 2.74e+00 lr: 4.14e-05:   2% 3/157 [00:03<02:48,  1.09s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 4.12e-05:   3% 4/157 [00:04<02:46,  1.09s/it]\u001b[A\n",
            "Training loss: 2.62e+00 lr: 4.11e-05:   3% 5/157 [00:05<02:45,  1.09s/it]\u001b[A\n",
            "Training loss: 2.62e+00 lr: 4.10e-05:   4% 6/157 [00:06<02:44,  1.09s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 4.08e-05:   4% 7/157 [00:07<02:43,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 4.07e-05:   5% 8/157 [00:08<02:42,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 4.06e-05:   6% 9/157 [00:09<02:41,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 4.04e-05:   6% 10/157 [00:10<02:39,  1.09s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 4.03e-05:   7% 11/157 [00:11<02:38,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 4.02e-05:   8% 12/157 [00:13<02:37,  1.09s/it]\u001b[A\n",
            "Training loss: 2.51e+00 lr: 4.00e-05:   8% 13/157 [00:14<02:36,  1.09s/it]\u001b[A\n",
            "Training loss: 2.55e+00 lr: 3.99e-05:   9% 14/157 [00:15<02:36,  1.09s/it]\u001b[A\n",
            "Training loss: 2.56e+00 lr: 3.98e-05:  10% 15/157 [00:16<02:35,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.96e-05:  10% 16/157 [00:17<02:33,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.95e-05:  11% 17/157 [00:18<02:32,  1.09s/it]\u001b[A\n",
            "Training loss: 2.56e+00 lr: 3.94e-05:  11% 18/157 [00:19<02:31,  1.09s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 3.92e-05:  12% 19/157 [00:20<02:30,  1.09s/it]\u001b[A\n",
            "Training loss: 2.54e+00 lr: 3.91e-05:  13% 20/157 [00:21<02:29,  1.09s/it]\u001b[A\n",
            "Training loss: 2.56e+00 lr: 3.90e-05:  13% 21/157 [00:22<02:27,  1.09s/it]\u001b[A\n",
            "Training loss: 2.56e+00 lr: 3.88e-05:  14% 22/157 [00:23<02:27,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.87e-05:  15% 23/157 [00:25<02:25,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 3.86e-05:  15% 24/157 [00:26<02:24,  1.09s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 3.84e-05:  16% 25/157 [00:27<02:23,  1.09s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 3.83e-05:  17% 26/157 [00:28<02:22,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 3.82e-05:  17% 27/157 [00:29<02:21,  1.09s/it]\u001b[A\n",
            "Training loss: 2.51e+00 lr: 3.80e-05:  18% 28/157 [00:30<02:20,  1.09s/it]\u001b[A\n",
            "Training loss: 2.54e+00 lr: 3.79e-05:  18% 29/157 [00:31<02:19,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.78e-05:  19% 30/157 [00:32<02:18,  1.09s/it]\u001b[A\n",
            "Training loss: 2.67e+00 lr: 3.76e-05:  20% 31/157 [00:33<02:17,  1.09s/it]\u001b[A\n",
            "Training loss: 2.71e+00 lr: 3.75e-05:  20% 32/157 [00:34<02:16,  1.09s/it]\u001b[A\n",
            "Training loss: 2.67e+00 lr: 3.74e-05:  21% 33/157 [00:35<02:15,  1.09s/it]\u001b[A\n",
            "Training loss: 2.57e+00 lr: 3.72e-05:  22% 34/157 [00:37<02:13,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 3.71e-05:  22% 35/157 [00:38<02:12,  1.09s/it]\u001b[A\n",
            "Training loss: 2.66e+00 lr: 3.70e-05:  23% 36/157 [00:39<02:11,  1.09s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 3.68e-05:  24% 37/157 [00:40<02:11,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 3.67e-05:  24% 38/157 [00:41<02:10,  1.10s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 3.66e-05:  25% 39/157 [00:42<02:08,  1.09s/it]\u001b[A\n",
            "Training loss: 2.46e+00 lr: 3.64e-05:  25% 40/157 [00:43<02:07,  1.09s/it]\u001b[A\n",
            "Training loss: 2.43e+00 lr: 3.63e-05:  26% 41/157 [00:44<02:06,  1.09s/it]\u001b[A\n",
            "Training loss: 2.48e+00 lr: 3.62e-05:  27% 42/157 [00:45<02:05,  1.09s/it]\u001b[A\n",
            "Training loss: 2.54e+00 lr: 3.60e-05:  27% 43/157 [00:46<02:04,  1.09s/it]\u001b[A\n",
            "Training loss: 2.54e+00 lr: 3.59e-05:  28% 44/157 [00:47<02:03,  1.09s/it]\u001b[A\n",
            "Training loss: 2.62e+00 lr: 3.58e-05:  29% 45/157 [00:49<02:02,  1.10s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 3.56e-05:  29% 46/157 [00:50<02:01,  1.10s/it]\u001b[A\n",
            "Training loss: 2.62e+00 lr: 3.55e-05:  30% 47/157 [00:51<02:00,  1.10s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.54e-05:  31% 48/157 [00:52<01:59,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.52e-05:  31% 49/157 [00:53<01:57,  1.09s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 3.51e-05:  32% 50/157 [00:54<01:56,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.50e-05:  32% 51/157 [00:55<01:55,  1.09s/it]\u001b[A\n",
            "Training loss: 2.56e+00 lr: 3.48e-05:  33% 52/157 [00:56<01:54,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 3.47e-05:  34% 53/157 [00:57<01:53,  1.09s/it]\u001b[A\n",
            "Training loss: 2.55e+00 lr: 3.46e-05:  34% 54/157 [00:58<01:52,  1.10s/it]\u001b[A\n",
            "Training loss: 2.57e+00 lr: 3.44e-05:  35% 55/157 [01:00<01:52,  1.10s/it]\u001b[A\n",
            "Training loss: 2.50e+00 lr: 3.43e-05:  36% 56/157 [01:01<01:51,  1.10s/it]\u001b[A\n",
            "Training loss: 2.49e+00 lr: 3.42e-05:  36% 57/157 [01:02<01:50,  1.10s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 3.40e-05:  37% 58/157 [01:03<01:48,  1.10s/it]\u001b[A\n",
            "Training loss: 2.73e+00 lr: 3.39e-05:  38% 59/157 [01:04<01:47,  1.10s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 3.38e-05:  38% 60/157 [01:05<01:46,  1.10s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.36e-05:  39% 61/157 [01:06<01:45,  1.10s/it]\u001b[A\n",
            "Training loss: 2.56e+00 lr: 3.35e-05:  39% 62/157 [01:07<01:44,  1.10s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 3.34e-05:  40% 63/157 [01:08<01:42,  1.09s/it]\u001b[A\n",
            "Training loss: 2.62e+00 lr: 3.32e-05:  41% 64/157 [01:09<01:41,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 3.31e-05:  41% 65/157 [01:10<01:40,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 3.30e-05:  42% 66/157 [01:12<01:39,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 3.28e-05:  43% 67/157 [01:13<01:38,  1.09s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 3.27e-05:  43% 68/157 [01:14<01:37,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.26e-05:  44% 69/157 [01:15<01:36,  1.09s/it]\u001b[A\n",
            "Training loss: 2.62e+00 lr: 3.24e-05:  45% 70/157 [01:16<01:35,  1.09s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 3.23e-05:  45% 71/157 [01:17<01:34,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.22e-05:  46% 72/157 [01:18<01:32,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 3.20e-05:  46% 73/157 [01:19<01:31,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 3.19e-05:  47% 74/157 [01:20<01:30,  1.09s/it]\u001b[A\n",
            "Training loss: 2.67e+00 lr: 3.18e-05:  48% 75/157 [01:21<01:29,  1.09s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 3.16e-05:  48% 76/157 [01:22<01:28,  1.10s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 3.15e-05:  49% 77/157 [01:24<01:27,  1.10s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 3.14e-05:  50% 78/157 [01:25<01:26,  1.10s/it]\u001b[A\n",
            "Training loss: 2.67e+00 lr: 3.12e-05:  50% 79/157 [01:26<01:25,  1.10s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 3.11e-05:  51% 80/157 [01:27<01:24,  1.09s/it]\u001b[A\n",
            "Training loss: 2.52e+00 lr: 3.10e-05:  52% 81/157 [01:28<01:22,  1.09s/it]\u001b[A\n",
            "Training loss: 2.52e+00 lr: 3.08e-05:  52% 82/157 [01:29<01:21,  1.09s/it]\u001b[A\n",
            "Training loss: 2.51e+00 lr: 3.07e-05:  53% 83/157 [01:30<01:20,  1.09s/it]\u001b[A\n",
            "Training loss: 2.51e+00 lr: 3.06e-05:  54% 84/157 [01:31<01:19,  1.09s/it]\u001b[A\n",
            "Training loss: 2.50e+00 lr: 3.04e-05:  54% 85/157 [01:32<01:18,  1.09s/it]\u001b[A\n",
            "Training loss: 2.57e+00 lr: 3.03e-05:  55% 86/157 [01:33<01:17,  1.10s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 3.02e-05:  55% 87/157 [01:35<01:16,  1.10s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 3.00e-05:  56% 88/157 [01:36<01:16,  1.10s/it]\u001b[A\n",
            "Training loss: 2.71e+00 lr: 2.99e-05:  57% 89/157 [01:37<01:14,  1.10s/it]\u001b[A\n",
            "Training loss: 2.68e+00 lr: 2.98e-05:  57% 90/157 [01:38<01:13,  1.10s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.97e-05:  58% 91/157 [01:39<01:12,  1.10s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 2.95e-05:  59% 92/157 [01:40<01:11,  1.10s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 2.94e-05:  59% 93/157 [01:41<01:10,  1.10s/it]\u001b[A\n",
            "Training loss: 2.57e+00 lr: 2.93e-05:  60% 94/157 [01:42<01:08,  1.10s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.91e-05:  61% 95/157 [01:43<01:07,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.90e-05:  61% 96/157 [01:44<01:06,  1.09s/it]\u001b[A\n",
            "Training loss: 2.58e+00 lr: 2.89e-05:  62% 97/157 [01:45<01:05,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 2.87e-05:  62% 98/157 [01:47<01:04,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 2.86e-05:  63% 99/157 [01:48<01:03,  1.09s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 2.85e-05:  64% 100/157 [01:49<01:02,  1.09s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 2.83e-05:  64% 101/157 [01:50<01:01,  1.10s/it]\u001b[A\n",
            "Training loss: 2.57e+00 lr: 2.82e-05:  65% 102/157 [01:51<01:00,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 2.81e-05:  66% 103/157 [01:52<00:59,  1.09s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 2.79e-05:  66% 104/157 [01:53<00:57,  1.09s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 2.78e-05:  67% 105/157 [01:54<00:56,  1.09s/it]\u001b[A\n",
            "Training loss: 2.60e+00 lr: 2.77e-05:  68% 106/157 [01:55<00:55,  1.09s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 2.75e-05:  68% 107/157 [01:56<00:54,  1.09s/it]\u001b[A\n",
            "Training loss: 2.67e+00 lr: 2.74e-05:  69% 108/157 [01:58<00:53,  1.10s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 2.73e-05:  69% 109/157 [01:59<00:52,  1.10s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 2.71e-05:  70% 110/157 [02:00<00:51,  1.10s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 2.70e-05:  71% 111/157 [02:01<00:50,  1.10s/it]\u001b[A\n",
            "Training loss: 2.65e+00 lr: 2.69e-05:  71% 112/157 [02:02<00:49,  1.10s/it]\u001b[A\n",
            "Training loss: 2.67e+00 lr: 2.67e-05:  72% 113/157 [02:03<00:48,  1.09s/it]\u001b[A\n",
            "Training loss: 2.66e+00 lr: 2.66e-05:  73% 114/157 [02:04<00:47,  1.09s/it]\u001b[A\n",
            "Training loss: 2.66e+00 lr: 2.65e-05:  73% 115/157 [02:05<00:45,  1.09s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 2.63e-05:  74% 116/157 [02:06<00:44,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.62e-05:  75% 117/157 [02:07<00:43,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.61e-05:  75% 118/157 [02:08<00:42,  1.09s/it]\u001b[A\n",
            "Training loss: 2.66e+00 lr: 2.59e-05:  76% 119/157 [02:10<00:41,  1.09s/it]\u001b[A\n",
            "Training loss: 2.55e+00 lr: 2.58e-05:  76% 120/157 [02:11<00:40,  1.10s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 2.57e-05:  77% 121/157 [02:12<00:39,  1.10s/it]\u001b[A\n",
            "Training loss: 2.56e+00 lr: 2.55e-05:  78% 122/157 [02:13<00:38,  1.10s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 2.54e-05:  78% 123/157 [02:14<00:37,  1.10s/it]\u001b[A\n",
            "Training loss: 2.46e+00 lr: 2.53e-05:  79% 124/157 [02:15<00:36,  1.10s/it]\u001b[A\n",
            "Training loss: 2.47e+00 lr: 2.51e-05:  80% 125/157 [02:16<00:35,  1.10s/it]\u001b[A\n",
            "Training loss: 2.45e+00 lr: 2.50e-05:  80% 126/157 [02:17<00:33,  1.10s/it]\u001b[A\n",
            "Training loss: 2.45e+00 lr: 2.49e-05:  81% 127/157 [02:18<00:32,  1.09s/it]\u001b[A\n",
            "Training loss: 2.51e+00 lr: 2.47e-05:  82% 128/157 [02:19<00:31,  1.09s/it]\u001b[A\n",
            "Training loss: 2.52e+00 lr: 2.46e-05:  82% 129/157 [02:20<00:30,  1.09s/it]\u001b[A\n",
            "Training loss: 2.49e+00 lr: 2.45e-05:  83% 130/157 [02:22<00:29,  1.09s/it]\u001b[A\n",
            "Training loss: 2.52e+00 lr: 2.43e-05:  83% 131/157 [02:23<00:28,  1.09s/it]\u001b[A\n",
            "Training loss: 2.50e+00 lr: 2.42e-05:  84% 132/157 [02:24<00:27,  1.10s/it]\u001b[A\n",
            "Training loss: 2.51e+00 lr: 2.41e-05:  85% 133/157 [02:25<00:26,  1.10s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 2.39e-05:  85% 134/157 [02:26<00:25,  1.10s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 2.38e-05:  86% 135/157 [02:27<00:24,  1.09s/it]\u001b[A\n",
            "Training loss: 2.55e+00 lr: 2.37e-05:  87% 136/157 [02:28<00:22,  1.09s/it]\u001b[A\n",
            "Training loss: 2.55e+00 lr: 2.35e-05:  87% 137/157 [02:29<00:21,  1.08s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 2.34e-05:  88% 138/157 [02:30<00:20,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 2.33e-05:  89% 139/157 [02:31<00:19,  1.09s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 2.31e-05:  89% 140/157 [02:33<00:18,  1.09s/it]\u001b[A\n",
            "Training loss: 2.57e+00 lr: 2.30e-05:  90% 141/157 [02:34<00:17,  1.10s/it]\u001b[A\n",
            "Training loss: 2.51e+00 lr: 2.29e-05:  90% 142/157 [02:35<00:16,  1.10s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 2.27e-05:  91% 143/157 [02:36<00:15,  1.10s/it]\u001b[A\n",
            "Training loss: 2.68e+00 lr: 2.26e-05:  92% 144/157 [02:37<00:14,  1.09s/it]\u001b[A\n",
            "Training loss: 2.67e+00 lr: 2.25e-05:  92% 145/157 [02:38<00:13,  1.09s/it]\u001b[A\n",
            "Training loss: 2.63e+00 lr: 2.23e-05:  93% 146/157 [02:39<00:11,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.22e-05:  94% 147/157 [02:40<00:10,  1.09s/it]\u001b[A\n",
            "Training loss: 2.64e+00 lr: 2.21e-05:  94% 148/157 [02:41<00:09,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.19e-05:  95% 149/157 [02:42<00:08,  1.09s/it]\u001b[A\n",
            "Training loss: 2.62e+00 lr: 2.18e-05:  96% 150/157 [02:43<00:07,  1.09s/it]\u001b[A\n",
            "Training loss: 2.61e+00 lr: 2.17e-05:  96% 151/157 [02:45<00:06,  1.09s/it]\u001b[A\n",
            "Training loss: 2.46e+00 lr: 2.15e-05:  97% 152/157 [02:46<00:05,  1.09s/it]\u001b[A\n",
            "Training loss: 2.50e+00 lr: 2.14e-05:  97% 153/157 [02:47<00:04,  1.09s/it]\u001b[A\n",
            "Training loss: 2.54e+00 lr: 2.13e-05:  98% 154/157 [02:48<00:03,  1.09s/it]\u001b[A\n",
            "Training loss: 2.53e+00 lr: 2.11e-05:  99% 155/157 [02:49<00:02,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 2.10e-05:  99% 156/157 [02:50<00:01,  1.09s/it]\u001b[A\n",
            "Training loss: 2.59e+00 lr: 2.09e-05: 100% 157/157 [02:51<00:00,  1.09s/it]\u001b[A\n",
            "Epoch:  67% 2/3 [05:42<02:51, 171.43s/it]\n",
            "Training:   0% 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Training loss: 2.44e+00 lr: 2.07e-05:   1% 1/157 [00:01<02:49,  1.09s/it]\u001b[A\n",
            "Training loss: 2.35e+00 lr: 2.06e-05:   1% 2/157 [00:02<02:48,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 2.05e-05:   2% 3/157 [00:03<02:47,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 2.03e-05:   3% 4/157 [00:04<02:46,  1.09s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 2.02e-05:   3% 5/157 [00:05<02:45,  1.09s/it]\u001b[A\n",
            "Training loss: 2.33e+00 lr: 2.01e-05:   4% 6/157 [00:06<02:44,  1.09s/it]\u001b[A\n",
            "Training loss: 2.33e+00 lr: 1.99e-05:   4% 7/157 [00:07<02:43,  1.09s/it]\u001b[A\n",
            "Training loss: 2.35e+00 lr: 1.98e-05:   5% 8/157 [00:08<02:43,  1.10s/it]\u001b[A\n",
            "Training loss: 2.36e+00 lr: 1.97e-05:   6% 9/157 [00:09<02:42,  1.10s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 1.95e-05:   6% 10/157 [00:10<02:41,  1.10s/it]\u001b[A\n",
            "Training loss: 2.35e+00 lr: 1.94e-05:   7% 11/157 [00:12<02:40,  1.10s/it]\u001b[A\n",
            "Training loss: 2.34e+00 lr: 1.93e-05:   8% 12/157 [00:13<02:39,  1.10s/it]\u001b[A\n",
            "Training loss: 2.38e+00 lr: 1.91e-05:   8% 13/157 [00:14<02:37,  1.10s/it]\u001b[A\n",
            "Training loss: 2.35e+00 lr: 1.90e-05:   9% 14/157 [00:15<02:36,  1.10s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 1.89e-05:  10% 15/157 [00:16<02:35,  1.10s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 1.87e-05:  10% 16/157 [00:17<02:33,  1.09s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 1.86e-05:  11% 17/157 [00:18<02:32,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 1.85e-05:  11% 18/157 [00:19<02:31,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 1.83e-05:  12% 19/157 [00:20<02:30,  1.09s/it]\u001b[A\n",
            "Training loss: 2.31e+00 lr: 1.82e-05:  13% 20/157 [00:21<02:29,  1.09s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.81e-05:  13% 21/157 [00:22<02:28,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 1.79e-05:  14% 22/157 [00:24<02:27,  1.09s/it]\u001b[A\n",
            "Training loss: 2.26e+00 lr: 1.78e-05:  15% 23/157 [00:25<02:25,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 1.77e-05:  15% 24/157 [00:26<02:24,  1.09s/it]\u001b[A\n",
            "Training loss: 2.31e+00 lr: 1.76e-05:  16% 25/157 [00:27<02:23,  1.09s/it]\u001b[A\n",
            "Training loss: 2.36e+00 lr: 1.74e-05:  17% 26/157 [00:28<02:23,  1.09s/it]\u001b[A\n",
            "Training loss: 2.36e+00 lr: 1.73e-05:  17% 27/157 [00:29<02:22,  1.09s/it]\u001b[A\n",
            "Training loss: 2.35e+00 lr: 1.72e-05:  18% 28/157 [00:30<02:21,  1.10s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 1.70e-05:  18% 29/157 [00:31<02:20,  1.09s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 1.69e-05:  19% 30/157 [00:32<02:19,  1.10s/it]\u001b[A\n",
            "Training loss: 2.35e+00 lr: 1.68e-05:  20% 31/157 [00:33<02:18,  1.10s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 1.66e-05:  20% 32/157 [00:34<02:16,  1.09s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 1.65e-05:  21% 33/157 [00:36<02:14,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 1.64e-05:  22% 34/157 [00:37<02:13,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 1.62e-05:  22% 35/157 [00:38<02:13,  1.09s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 1.61e-05:  23% 36/157 [00:39<02:12,  1.10s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.60e-05:  24% 37/157 [00:40<02:11,  1.09s/it]\u001b[A\n",
            "Training loss: 2.26e+00 lr: 1.58e-05:  24% 38/157 [00:41<02:10,  1.10s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 1.57e-05:  25% 39/157 [00:42<02:09,  1.10s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.56e-05:  25% 40/157 [00:43<02:08,  1.10s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 1.54e-05:  26% 41/157 [00:44<02:08,  1.10s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 1.53e-05:  27% 42/157 [00:45<02:07,  1.10s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 1.52e-05:  27% 43/157 [00:47<02:06,  1.11s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 1.50e-05:  28% 44/157 [00:48<02:04,  1.10s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 1.49e-05:  29% 45/157 [00:49<02:03,  1.10s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 1.48e-05:  29% 46/157 [00:50<02:01,  1.10s/it]\u001b[A\n",
            "Training loss: 2.26e+00 lr: 1.46e-05:  30% 47/157 [00:51<02:00,  1.10s/it]\u001b[A\n",
            "Training loss: 2.26e+00 lr: 1.45e-05:  31% 48/157 [00:52<01:59,  1.10s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 1.44e-05:  31% 49/157 [00:53<01:58,  1.09s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.42e-05:  32% 50/157 [00:54<01:56,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 1.41e-05:  32% 51/157 [00:55<01:55,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 1.40e-05:  33% 52/157 [00:56<01:54,  1.09s/it]\u001b[A\n",
            "Training loss: 2.33e+00 lr: 1.38e-05:  34% 53/157 [00:57<01:53,  1.09s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 1.37e-05:  34% 54/157 [00:59<01:52,  1.09s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.36e-05:  35% 55/157 [01:00<01:51,  1.09s/it]\u001b[A\n",
            "Training loss: 2.26e+00 lr: 1.34e-05:  36% 56/157 [01:01<01:50,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 1.33e-05:  36% 57/157 [01:02<01:48,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 1.32e-05:  37% 58/157 [01:03<01:47,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 1.30e-05:  38% 59/157 [01:04<01:46,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 1.29e-05:  38% 60/157 [01:05<01:45,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 1.28e-05:  39% 61/157 [01:06<01:44,  1.09s/it]\u001b[A\n",
            "Training loss: 2.21e+00 lr: 1.26e-05:  39% 62/157 [01:07<01:44,  1.10s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 1.25e-05:  40% 63/157 [01:08<01:42,  1.10s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 1.24e-05:  41% 64/157 [01:10<01:41,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 1.22e-05:  41% 65/157 [01:11<01:40,  1.09s/it]\u001b[A\n",
            "Training loss: 2.17e+00 lr: 1.21e-05:  42% 66/157 [01:12<01:39,  1.09s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.20e-05:  43% 67/157 [01:13<01:38,  1.09s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 1.18e-05:  43% 68/157 [01:14<01:37,  1.10s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 1.17e-05:  44% 69/157 [01:15<01:36,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 1.16e-05:  45% 70/157 [01:16<01:35,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 1.14e-05:  45% 71/157 [01:17<01:33,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 1.13e-05:  46% 72/157 [01:18<01:33,  1.10s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.12e-05:  46% 73/157 [01:19<01:32,  1.10s/it]\u001b[A\n",
            "Training loss: 2.26e+00 lr: 1.10e-05:  47% 74/157 [01:20<01:30,  1.10s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 1.09e-05:  48% 75/157 [01:22<01:29,  1.10s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 1.08e-05:  48% 76/157 [01:23<01:28,  1.10s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 1.06e-05:  49% 77/157 [01:24<01:27,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 1.05e-05:  50% 78/157 [01:25<01:26,  1.09s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.04e-05:  50% 79/157 [01:26<01:25,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 1.02e-05:  51% 80/157 [01:27<01:24,  1.09s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 1.01e-05:  52% 81/157 [01:28<01:23,  1.10s/it]\u001b[A\n",
            "Training loss: 2.34e+00 lr: 9.97e-06:  52% 82/157 [01:29<01:22,  1.10s/it]\u001b[A\n",
            "Training loss: 2.33e+00 lr: 9.84e-06:  53% 83/157 [01:30<01:21,  1.10s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 9.71e-06:  54% 84/157 [01:31<01:19,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 9.57e-06:  54% 85/157 [01:32<01:18,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 9.44e-06:  55% 86/157 [01:34<01:17,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 9.31e-06:  55% 87/157 [01:35<01:16,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 9.17e-06:  56% 88/157 [01:36<01:15,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 9.04e-06:  57% 89/157 [01:37<01:14,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 8.91e-06:  57% 90/157 [01:38<01:13,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 8.78e-06:  58% 91/157 [01:39<01:12,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 8.64e-06:  59% 92/157 [01:40<01:11,  1.10s/it]\u001b[A\n",
            "Training loss: 2.20e+00 lr: 8.51e-06:  59% 93/157 [01:41<01:10,  1.10s/it]\u001b[A\n",
            "Training loss: 2.17e+00 lr: 8.38e-06:  60% 94/157 [01:42<01:09,  1.10s/it]\u001b[A\n",
            "Training loss: 2.20e+00 lr: 8.24e-06:  61% 95/157 [01:43<01:08,  1.10s/it]\u001b[A\n",
            "Training loss: 2.18e+00 lr: 8.11e-06:  61% 96/157 [01:45<01:06,  1.10s/it]\u001b[A\n",
            "Training loss: 2.21e+00 lr: 7.98e-06:  62% 97/157 [01:46<01:05,  1.09s/it]\u001b[A\n",
            "Training loss: 2.16e+00 lr: 7.84e-06:  62% 98/157 [01:47<01:04,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 7.71e-06:  63% 99/157 [01:48<01:03,  1.09s/it]\u001b[A\n",
            "Training loss: 2.17e+00 lr: 7.58e-06:  64% 100/157 [01:49<01:02,  1.10s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 7.45e-06:  64% 101/157 [01:50<01:01,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 7.31e-06:  65% 102/157 [01:51<01:00,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 7.18e-06:  66% 103/157 [01:52<00:58,  1.09s/it]\u001b[A\n",
            "Training loss: 2.21e+00 lr: 7.05e-06:  66% 104/157 [01:53<00:58,  1.09s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 6.91e-06:  67% 105/157 [01:54<00:57,  1.10s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 6.78e-06:  68% 106/157 [01:55<00:55,  1.09s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 6.65e-06:  68% 107/157 [01:57<00:54,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 6.52e-06:  69% 108/157 [01:58<00:53,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 6.38e-06:  69% 109/157 [01:59<00:52,  1.09s/it]\u001b[A\n",
            "Training loss: 2.18e+00 lr: 6.25e-06:  70% 110/157 [02:00<00:51,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 6.12e-06:  71% 111/157 [02:01<00:50,  1.09s/it]\u001b[A\n",
            "Training loss: 2.16e+00 lr: 5.98e-06:  71% 112/157 [02:02<00:49,  1.09s/it]\u001b[A\n",
            "Training loss: 2.19e+00 lr: 5.85e-06:  72% 113/157 [02:03<00:48,  1.09s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 5.72e-06:  73% 114/157 [02:04<00:46,  1.09s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 5.58e-06:  73% 115/157 [02:05<00:45,  1.09s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 5.45e-06:  74% 116/157 [02:06<00:44,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 5.32e-06:  75% 117/157 [02:07<00:43,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 5.19e-06:  75% 118/157 [02:09<00:42,  1.09s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 5.05e-06:  76% 119/157 [02:10<00:41,  1.09s/it]\u001b[A\n",
            "Training loss: 2.21e+00 lr: 4.92e-06:  76% 120/157 [02:11<00:40,  1.09s/it]\u001b[A\n",
            "Training loss: 2.18e+00 lr: 4.79e-06:  77% 121/157 [02:12<00:39,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 4.65e-06:  78% 122/157 [02:13<00:38,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 4.52e-06:  78% 123/157 [02:14<00:37,  1.09s/it]\u001b[A\n",
            "Training loss: 2.26e+00 lr: 4.39e-06:  79% 124/157 [02:15<00:36,  1.09s/it]\u001b[A\n",
            "Training loss: 2.21e+00 lr: 4.25e-06:  80% 125/157 [02:16<00:35,  1.10s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 4.12e-06:  80% 126/157 [02:17<00:33,  1.10s/it]\u001b[A\n",
            "Training loss: 2.24e+00 lr: 3.99e-06:  81% 127/157 [02:18<00:32,  1.10s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 3.86e-06:  82% 128/157 [02:19<00:31,  1.10s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 3.72e-06:  82% 129/157 [02:21<00:30,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 3.59e-06:  83% 130/157 [02:22<00:29,  1.09s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 3.46e-06:  83% 131/157 [02:23<00:28,  1.09s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 3.32e-06:  84% 132/157 [02:24<00:27,  1.09s/it]\u001b[A\n",
            "Training loss: 2.37e+00 lr: 3.19e-06:  85% 133/157 [02:25<00:26,  1.09s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 3.06e-06:  85% 134/157 [02:26<00:25,  1.10s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 2.93e-06:  86% 135/157 [02:27<00:24,  1.09s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 2.79e-06:  87% 136/157 [02:28<00:22,  1.09s/it]\u001b[A\n",
            "Training loss: 2.36e+00 lr: 2.66e-06:  87% 137/157 [02:29<00:21,  1.10s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 2.53e-06:  88% 138/157 [02:30<00:20,  1.10s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 2.39e-06:  89% 139/157 [02:31<00:19,  1.09s/it]\u001b[A\n",
            "Training loss: 2.22e+00 lr: 2.26e-06:  89% 140/157 [02:33<00:18,  1.09s/it]\u001b[A\n",
            "Training loss: 2.15e+00 lr: 2.13e-06:  90% 141/157 [02:34<00:17,  1.09s/it]\u001b[A\n",
            "Training loss: 2.23e+00 lr: 1.99e-06:  90% 142/157 [02:35<00:16,  1.09s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 1.86e-06:  91% 143/157 [02:36<00:15,  1.09s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.73e-06:  92% 144/157 [02:37<00:14,  1.09s/it]\u001b[A\n",
            "Training loss: 2.27e+00 lr: 1.60e-06:  92% 145/157 [02:38<00:13,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 1.46e-06:  93% 146/157 [02:39<00:12,  1.10s/it]\u001b[A\n",
            "Training loss: 2.20e+00 lr: 1.33e-06:  94% 147/157 [02:40<00:10,  1.09s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 1.20e-06:  94% 148/157 [02:41<00:09,  1.09s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 1.06e-06:  95% 149/157 [02:42<00:08,  1.09s/it]\u001b[A\n",
            "Training loss: 2.28e+00 lr: 9.31e-07:  96% 150/157 [02:43<00:07,  1.09s/it]\u001b[A\n",
            "Training loss: 2.31e+00 lr: 7.98e-07:  96% 151/157 [02:45<00:06,  1.09s/it]\u001b[A\n",
            "Training loss: 2.30e+00 lr: 6.65e-07:  97% 152/157 [02:46<00:05,  1.10s/it]\u001b[A\n",
            "Training loss: 2.32e+00 lr: 5.32e-07:  97% 153/157 [02:47<00:04,  1.10s/it]\u001b[A\n",
            "Training loss: 2.25e+00 lr: 3.99e-07:  98% 154/157 [02:48<00:03,  1.10s/it]\u001b[A\n",
            "Training loss: 2.31e+00 lr: 2.66e-07:  99% 155/157 [02:49<00:02,  1.10s/it]\u001b[A\n",
            "Training loss: 2.29e+00 lr: 1.33e-07:  99% 156/157 [02:50<00:01,  1.10s/it]\u001b[A\n",
            "Training loss: 2.21e+00 lr: -0.00e+00: 100% 157/157 [02:51<00:00,  1.10s/it]\u001b[A\n",
            "Epoch: 100% 3/3 [08:34<00:00, 171.50s/it]\n",
            "07/08/2019 13:31:11 - INFO - pytorch_pretrained_bert.modeling_openai -   loading weights file train_clarke/pytorch_model.bin\n",
            "07/08/2019 13:31:11 - INFO - pytorch_pretrained_bert.modeling_openai -   loading configuration file train_clarke/config.json\n",
            "07/08/2019 13:31:11 - INFO - pytorch_pretrained_bert.modeling_openai -   Model config {\n",
            "  \"afn\": \"gelu\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 512,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 512,\n",
            "  \"n_special\": 2,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"vocab_size\": 40478\n",
            "}\n",
            "\n",
            "07/08/2019 13:31:14 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading special tokens file train_clarke/special_tokens.txt\n",
            "07/08/2019 13:31:14 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file train_clarke/vocab.json\n",
            "07/08/2019 13:31:14 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file train_clarke/merges.txt\n",
            "07/08/2019 13:31:14 - INFO - pytorch_pretrained_bert.tokenization_openai -   Special tokens {'_start_': 40478, '_end_': 40479}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFWa2On1yoZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "b49437f5-741d-4f69-8778-5852a55458bd"
      },
      "source": [
        "!python Text-Generation/openai_huggingface_example.py --model_name_or_path \"train_clarke\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/08/2019 13:35:22 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading special tokens file train_clarke/special_tokens.txt\n",
            "07/08/2019 13:35:22 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file train_clarke/vocab.json\n",
            "07/08/2019 13:35:22 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file train_clarke/merges.txt\n",
            "07/08/2019 13:35:23 - INFO - pytorch_pretrained_bert.tokenization_openai -   Special tokens {'_start_': 40478, '_end_': 40479}\n",
            "07/08/2019 13:35:23 - INFO - pytorch_pretrained_bert.modeling_openai -   loading weights file train_clarke/pytorch_model.bin\n",
            "07/08/2019 13:35:23 - INFO - pytorch_pretrained_bert.modeling_openai -   loading configuration file train_clarke/config.json\n",
            "07/08/2019 13:35:23 - INFO - pytorch_pretrained_bert.modeling_openai -   Model config {\n",
            "  \"afn\": \"gelu\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 512,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 512,\n",
            "  \"n_special\": 2,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"vocab_size\": 40478\n",
            "}\n",
            "\n",
            "07/08/2019 13:35:26 - INFO - pytorch_pretrained_bert.modeling_openai -   Weights from pretrained model not used in OpenAIGPTLMHeadModel: ['multiple_choice_head.linear.weight', 'multiple_choice_head.linear.bias']\n",
            "Model prompt >>> Dr . Floyd was really angry because\n",
            "100% 256/256 [00:09<00:00, 16.56it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "the party must have held up late. he opened the hatch briefly, and then, with more caution than confidence, stepped out into the night he had once inhabited, back across the crater of earth, and back into solid space. _end_... on the dark sides of the mountains and valleys... ( this had happened again, more times than he could count. ) _end__end_... _end_that was improvicomspheres of the satellites, staring out their movements, until they prickled the worst of glory and then, as the mist moved across the sky. _end__end__end__end_one hour, gradually transparrends the void, then continued along the air. _end_even cautiously improving its dominion over the wild without resistance, until the rhythms crept out upon the black emptiness. for this did not allow the loneliness of time drove itself to eternity. that had been coded with turbulent infinities of identity. _end_day by night, before descending into complacment, until it had become nothing but permanent. _end__end_... \n",
            " his ears, listening to the stars beyond his senses. _end_nuclear freeze ; in concentration. only the world, until the mini suns passed across the empty self, and as it had found its limits. he did not know why he loosed itself off its rhythm. but the night had the same\n",
            "================================================================================\n",
            "Model prompt >>> I heard some voices with\n",
            "100% 256/256 [00:09<00:00, 17.04it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "difficulty, as if the body was being carried around by a tremendous washing machine, pumping out vast outputs of hot air. bowman watched the machine in fascinated fascination ; he felt as if he could breathe it. _end_... the laws were the same ; yet he had never truly known them. _end_even more baffling, as the breathing was hurled aside like lightning through the darkness, was one of infinite possibilities. _end_... the what else was deeper and deeper, the ceaseless grind of drifting impulses... _end_... the long - still discovery immersed him, discovery's first mortifying eruptions of bowels. _end_man discovered resistance : sudden bombing, peredogs - possible the effect of infinite rapture. _end_... and then, like malice ; then human habitability, acute exploration. then phansis, isolation ; and ultimately, _end__end_the watch after victory over the planet earth, voyeur gs - aching exercise retribush - disappointment the retreat ; retreat from achievement, vigilance toward rock - face this and finally, collapse into silence. _end__end_tanks, retreat from deep seas of fury... as lonely and eternal. _end__end_kill the dream. they might die in utter silence ; crystal canoes filled with fire. _end_... retreat, retreat - this world empty. _end_of dullife... the world of dreams ; the\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"Text-Generation/openai_huggingface_example.py\", line 140, in <module>\n",
            "    run_model()\n",
            "  File \"Text-Generation/openai_huggingface_example.py\", line 97, in run_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}